{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Umamrozaq/streamlit-echarts-demo/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suoA2BJEGKlO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G8F7xMCLZUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataco_supply_chain = pd.read_csv('/content/drive/MyDrive/supply chain forecast/dataset/DataCoSupplyChainDataset.csv', encoding='ISO-8859-1')\n",
        "dataco_supply_chain.head()"
      ],
      "metadata": {
        "id": "lLZLs8lPn8Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation & Cleaning"
      ],
      "metadata": {
        "id": "EhGgKJrguUzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ubah colom menjadi huruf kapital dan ubah spasi menjadi _\n",
        "dtype='category'\n",
        "dataco_supply_chain.columns = dataco_supply_chain.columns.str.upper().str.replace(' ', '_')\n",
        "dataco_supply_chain.head()"
      ],
      "metadata": {
        "id": "zinmCN9JuMSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataco_supply_chain.info()"
      ],
      "metadata": {
        "id": "1K5AMtXnzMdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## pilih kolom yang digunakan\n",
        "\n",
        "dataco_supply_chain = dataco_supply_chain[\n",
        "    ['ORDER_DATE_(DATEORDERS)'\n",
        "     , 'CATEGORY_NAME','CATEGORY_ID'\n",
        "     ,'ORDER_ITEM_QUANTITY'\n",
        "     ,'ORDER_REGION'\n",
        "     ,'ORDER_STATUS'\n",
        "     ,'PRODUCT_NAME','PRODUCT_CARD_ID'\n",
        "     ,'DAYS_FOR_SHIPPING_(REAL)','DAYS_FOR_SHIPMENT_(SCHEDULED)'\n",
        "     ,\n",
        "    ]\n",
        "]"
      ],
      "metadata": {
        "id": "L2h0p6DYzTQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataco_supply_chain.head()"
      ],
      "metadata": {
        "id": "t_j9vnB1Y-mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataco_supply_chain.info()"
      ],
      "metadata": {
        "id": "GPi3q6Nuz6fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataco_supply_chain['ORDER_DATE_(DATEORDERS)'] = pd.to_datetime(dataco_supply_chain['ORDER_DATE_(DATEORDERS)'])\n",
        "dataco_supply_chain.info()"
      ],
      "metadata": {
        "id": "fGQTYLoW0kmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Transformation"
      ],
      "metadata": {
        "id": "TkQ9e16d1CJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting year, month, day, and weekday from the order date\n",
        "dataco_supply_chain['ORDER_YEAR'] = dataco_supply_chain['ORDER_DATE_(DATEORDERS)'].dt.year\n",
        "dataco_supply_chain['ORDER_MONTH'] = dataco_supply_chain['ORDER_DATE_(DATEORDERS)'].dt.month\n",
        "dataco_supply_chain['ORDER_DAY'] = dataco_supply_chain['ORDER_DATE_(DATEORDERS)'].dt.day\n",
        "dataco_supply_chain['ORDER_WEEKDAY'] = dataco_supply_chain['ORDER_DATE_(DATEORDERS)'].dt.weekday\n",
        "dataco_supply_chain['ORDER_DATE'] = dataco_supply_chain['ORDER_DATE_(DATEORDERS)'].dt.date\n",
        "dataco_supply_chain.drop(columns='ORDER_DATE_(DATEORDERS)', inplace=True)\n",
        "dataco_supply_chain.info()"
      ],
      "metadata": {
        "id": "PQvArtOV042n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "pyAOxGwY3IBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataco_supply_chain.describe(datetime_is_numeric=True).round(2)"
      ],
      "metadata": {
        "id": "WGupIDQI1W8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of ORDER_ITEM_QUANTITY\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.displot(dataco_supply_chain['ORDER_ITEM_QUANTITY'])\n",
        "plt.title('Distribution of Order Item Quantity')\n",
        "plt.show()\n",
        "\n",
        "# Orders over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "dataco_supply_chain.groupby('ORDER_DATE')['ORDER_ITEM_QUANTITY'].sum().plot()\n",
        "plt.title('Total Orders Over Time')\n",
        "plt.ylabel('Total Order Quantity')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of orders by region\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=dataco_supply_chain, y='ORDER_REGION')\n",
        "plt.title('Distribution of Orders by Region')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of orders by category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=dataco_supply_chain, y='CATEGORY_NAME')\n",
        "plt.title('Distribution of Orders by Category')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of order status\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=dataco_supply_chain, y='ORDER_STATUS')\n",
        "plt.title('Distribution of Order Status')\n",
        "plt.show()\n",
        "\n",
        "# Average days for real shipping vs scheduled shipping\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=dataco_supply_chain, x='ORDER_REGION', y='DAYS_FOR_SHIPPING_(REAL)', color='blue', label='Real Shipping Days')\n",
        "sns.barplot(data=dataco_supply_chain, x='ORDER_REGION', y='DAYS_FOR_SHIPMENT_(SCHEDULED)', color='red', alpha=0.5, label='Scheduled Shipping Days')\n",
        "plt.title('Average Days for Real vs Scheduled Shipping by Region')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(dataco_supply_chain.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gTPFKjEu1eR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orders over time\n",
        "plt.figure(figsize=(12, 8))\n",
        "daily_orders = dataco_supply_chain.groupby(['ORDER_DATE'])['ORDER_ITEM_QUANTITY'].sum().reset_index()\n",
        "sns.lineplot(data=daily_orders, x='ORDER_DATE', y='ORDER_ITEM_QUANTITY')\n",
        "plt.title('Total Order Quantity Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Quantity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TqD0BM0E16XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orders over time - filter out last dates\n",
        "plt.figure(figsize=(12, 8))\n",
        "filtered_daily_orders = daily_orders.copy()\n",
        "#filtered_daily_orders = daily_orders[(daily_orders['ORDER_DATE'] <= pd.to_datetime('2017-07-01'))]\n",
        "sns.lineplot(data=filtered_daily_orders, x='ORDER_DATE', y='ORDER_ITEM_QUANTITY')\n",
        "plt.title('Total Order Quantity Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Quantity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gazHTXIW2ADf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot for daily order quantities\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=filtered_daily_orders['ORDER_ITEM_QUANTITY'])\n",
        "plt.title('Boxplot of Daily Order Quantities')\n",
        "plt.xlabel('Total Order Quantity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zB_qpdmn2w5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove outliers\n",
        "# Aggregate order quantities by date\n",
        "\n",
        "# Calculate Q1, Q3, and IQR\n",
        "Q1 = filtered_daily_orders['ORDER_ITEM_QUANTITY'].quantile(0.25)\n",
        "Q3 = filtered_daily_orders['ORDER_ITEM_QUANTITY'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter out outliers\n",
        "cleaned_daily_orders = filtered_daily_orders[(filtered_daily_orders['ORDER_ITEM_QUANTITY'] >= lower_bound) &\n",
        "                               (filtered_daily_orders['ORDER_ITEM_QUANTITY'] <= upper_bound)]\n",
        "\n",
        "# Plotting the time series without outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(cleaned_daily_orders['ORDER_DATE'], cleaned_daily_orders['ORDER_ITEM_QUANTITY'])\n",
        "plt.title('Total Orders Over Time (Without Outliers)')\n",
        "plt.ylabel('Total Order Quantity')\n",
        "plt.show()\n",
        "\n",
        "# Boxplot for daily order quantities\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=cleaned_daily_orders['ORDER_ITEM_QUANTITY'])\n",
        "plt.title('Boxplot of Daily Order Quantities')\n",
        "plt.xlabel('Total Order Quantity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-MBmxgJQ2151"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create monthly, weekly data\n",
        "cleaned_daily_orders = cleaned_daily_orders.copy()\n",
        "cleaned_daily_orders['ORDER_DATE'] = pd.to_datetime(cleaned_daily_orders['ORDER_DATE'])\n",
        "cleaned_daily_orders['YEAR_MONTH']=cleaned_daily_orders['ORDER_DATE'].dt.to_period('M')\n",
        "cleaned_daily_orders['YEAR_WEEK']=cleaned_daily_orders['ORDER_DATE'].dt.to_period('W')\n",
        "cleaned_daily_orders"
      ],
      "metadata": {
        "id": "gTSL-QH_28uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_daily_orders.info()"
      ],
      "metadata": {
        "id": "_CVxHxFT3e8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'year-month' and sum the ORDER_ITEM_QUANTITY\n",
        "monthly_orders = cleaned_daily_orders.groupby('YEAR_MONTH')['ORDER_ITEM_QUANTITY'].sum()\n",
        "weekly_orders = cleaned_daily_orders.groupby('YEAR_WEEK')['ORDER_ITEM_QUANTITY'].sum()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "monthly_orders.plot(kind='line', marker='o')\n",
        "plt.title('Total Orders by Month')\n",
        "plt.ylabel('Total Order Quantity')\n",
        "plt.xlabel('Month')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "weekly_orders.plot(kind='line', marker='o')\n",
        "plt.title('Total Orders by Week')\n",
        "plt.ylabel('Total Order Quantity')\n",
        "plt.xlabel('Week')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hgwg4C93k9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_orders.info()"
      ],
      "metadata": {
        "id": "9uUqI6Dk3qn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demand forecasting\n",
        "\n",
        "Demand forecasting adalah proses peramalan atau prediksi permintaan (demand) suatu produk atau layanan di masa depan. Tujuan dari demand forecasting adalah untuk membantu perusahaan atau organisasi dalam perencanaan produksi, persediaan, distribusi, dan manajemen sumber daya lainnya dengan lebih efisien. Dengan informasi yang akurat tentang permintaan di masa depan, perusahaan dapat menghindari overstock atau understock barang, mengoptimalkan produksi, dan meningkatkan kepuasan pelanggan."
      ],
      "metadata": {
        "id": "sTDat9P34EU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet"
      ],
      "metadata": {
        "id": "yjeLpSsr4MwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Convert the series to a DataFrame\n",
        "weekly_orders_df = weekly_orders.reset_index()\n",
        "weekly_orders_df.columns = ['ds', 'y']\n",
        "\n",
        "# Convert the 'ds' column to datetime format\n",
        "weekly_orders_df['ds'] = weekly_orders_df['ds'].dt.to_timestamp()\n",
        "\n",
        "# Split the data (holding out the last 20% for testing)\n",
        "split_point = int(len(weekly_orders_df) * 0.80)\n",
        "train = weekly_orders_df.iloc[:split_point]\n",
        "test = weekly_orders_df.iloc[split_point:]\n",
        "\n",
        "# Initialize and fit the Prophet model\n",
        "weekly_model = Prophet()\n",
        "weekly_model.fit(train)\n",
        "\n",
        "# Create future dates for prediction (entire duration: train + test)\n",
        "weekly_future = weekly_model.make_future_dataframe(periods=len(test), freq='W-SUN')\n",
        "\n",
        "# Predict\n",
        "weekly_forecast = weekly_model.predict(weekly_future)"
      ],
      "metadata": {
        "id": "x6ciSmfU4bxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Training data\n",
        "y_pred_train = weekly_forecast['yhat'][:split_point]\n",
        "mae_train = mean_absolute_error(train['y'], y_pred_train)\n",
        "mse_train = mean_squared_error(train['y'], y_pred_train)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "\n",
        "print(f\"Training MAE: {mae_train}\")\n",
        "print(f\"Training MSE: {mse_train}\")\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "\n",
        "# Evaluate on Testing data\n",
        "y_pred_test = weekly_forecast['yhat'][split_point:]\n",
        "mae_test = mean_absolute_error(test['y'], y_pred_test)\n",
        "mse_test = mean_squared_error(test['y'], y_pred_test)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "\n",
        "print(f\"\\nTesting MAE: {mae_test}\")\n",
        "print(f\"Testing MSE: {mse_test}\")\n",
        "print(f\"Testing RMSE: {rmse_test}\")"
      ],
      "metadata": {
        "id": "7qiIMUZT4jXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the forecast\n",
        "weekly_fig1 = weekly_model.plot(weekly_forecast)\n",
        "plt.title('Weekly Orders Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Order Quantity')\n",
        "plt.show()\n",
        "\n",
        "# If you want to see the components of the forecast (trend, yearly seasonality, etc.)\n",
        "weekly_fig2 = weekly_model.plot_components(weekly_forecast)"
      ],
      "metadata": {
        "id": "M56kxK6a40oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Visualize actual versus predicted values.\n",
        "# Plotting actual vs predicted values\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(train['ds'], train['y'], label='Training Data', color='blue')\n",
        "plt.plot(test['ds'], test['y'], label='Actual Test Data', color='orange')\n",
        "plt.plot(train['ds'], y_pred_train, label='Predicted Training Data', color='red', linestyle='--')\n",
        "plt.plot(test['ds'], y_pred_test, label='Predicted Test Data', color='green', linestyle='--')\n",
        "plt.title('Actual vs Predicted Weekly Orders')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Order Quantity')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pKHr4Cg246R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_orders_df.describe()"
      ],
      "metadata": {
        "id": "p_8YzEA15EH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inventory management"
      ],
      "metadata": {
        "id": "1H1F1kbw5Mxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Merge 'y' values from train and test into weekly_forecast\n",
        "weekly_forecast = weekly_forecast.merge(train[['ds', 'y']], on='ds', how='left')\n",
        "weekly_forecast = weekly_forecast.merge(test[['ds', 'y']], on='ds', how='left', suffixes=('', '_test'))\n",
        "weekly_forecast['y'].fillna(weekly_forecast['y_test'], inplace=True)\n",
        "weekly_forecast.drop(columns='y_test', inplace=True)\n",
        "\n",
        "# Calculate weekly standard deviation of actual demand\n",
        "weekly_forecast['std_dev'] = weekly_forecast['y'].rolling(window=7).std()\n",
        "\n",
        "# Calculate weekly safety stock\n",
        "weekly_forecast['safety_stock'] = 1.65 * weekly_forecast['std_dev'] * np.sqrt(1)\n",
        "\n",
        "# Calculate weekly average demand from actual data\n",
        "weekly_forecast['avg_weekly_demand'] = weekly_forecast['y'].rolling(window=7).mean()\n",
        "\n",
        "# Calculate weekly reorder point\n",
        "weekly_forecast['reorder_point'] = (weekly_forecast['avg_weekly_demand'] * 1) + weekly_forecast['safety_stock']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['y'], label='Actual Demand', color='blue')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['yhat'], label='Predicted Demand', color='green')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['safety_stock'], label='Safety Stock', color='red', linestyle='--')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['reorder_point'], label='Reorder Point', color='orange', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Demand with Safety Stock and Reorder Point')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Order Quantity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UBvkB-nm5Yo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##With Forecasted Safety Stock and Reorder Point\n",
        "# Calculate weekly standard deviation of forecasted demand\n",
        "weekly_forecast['forecasted_std_dev'] = weekly_forecast['yhat'].rolling(window=7).std()\n",
        "\n",
        "# Calculate weekly safety stock for forecasted demand\n",
        "weekly_forecast['forecasted_safety_stock'] = 1.65 * weekly_forecast['forecasted_std_dev'] * np.sqrt(1)\n",
        "\n",
        "# Calculate weekly average demand from forecasted data\n",
        "weekly_forecast['forecasted_avg_weekly_demand'] = weekly_forecast['yhat'].rolling(window=7).mean()\n",
        "\n",
        "# Calculate weekly reorder point for forecasted demand\n",
        "weekly_forecast['forecasted_reorder_point'] = (weekly_forecast['forecasted_avg_weekly_demand'] * 1) + weekly_forecast['forecasted_safety_stock']\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['y'], label='Actual Demand', color='blue')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['yhat'], label='Predicted Demand', color='green')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['safety_stock'], label='Safety Stock (Actual)', color='red', linestyle='--')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['forecasted_safety_stock'], label='Safety Stock (Forecasted)', color='purple', linestyle='--')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['reorder_point'], label='Reorder Point (Actual)', color='orange', linestyle='--')\n",
        "plt.plot(weekly_forecast['ds'], weekly_forecast['forecasted_reorder_point'], label='Reorder Point (Forecasted)', color='pink', linestyle='--')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Demand with Safety Stock and Reorder Point')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Order Quantity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mzdEgbwd5cEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluaztion"
      ],
      "metadata": {
        "id": "dKBvNgBwwvWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Misalnya, Anda memiliki data aktual dan prediksi\n",
        "actual_data = [10, 20, 30, 40, 50]\n",
        "predicted_data = [12, 18, 28, 38, 52]\n",
        "\n",
        "# Menghitung r-squared (coefficient of determination)\n",
        "r2 = r2_score(actual_data, predicted_data)\n",
        "\n",
        "print(f\"R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "5B30CEcv0eMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset (e.g., Iris dataset)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (class labels)\n",
        "\n",
        "# Split the dataset into a training set and a testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "clf = SVC(kernel='linear')  # You can choose different kernels (e.g., 'linear', 'rbf', 'poly')\n",
        "\n",
        "# Train the SVM classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# You can now use 'y_pred' to make predictions on new data\n"
      ],
      "metadata": {
        "id": "-Fj-9JsQpYYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create the confusion matrix\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)"
      ],
      "metadata": {
        "id": "l217EYpxpkrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Compute precision\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # 'weighted' untuk multiclass classification\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "# Compute recall\n",
        "recall = recall_score(y_test, y_pred, average='weighted')  # 'weighted' untuk multiclass classification\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Compute F1 score\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' untuk multiclass classification\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "V3uXJWtZqAQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor  # Mengganti SVM dengan DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error  # Menggunakan mean_squared_error untuk regresi\n",
        "\n",
        "# Load a sample dataset (e.g., Iris dataset)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (class labels)\n",
        "\n",
        "# Split the dataset into a training set and a testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the Decision Tree Regressor on the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the regressor's performance using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# You can now use 'y_pred' to make predictions on new data\n"
      ],
      "metadata": {
        "id": "LWe5_uBNPQqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deployment"
      ],
      "metadata": {
        "id": "s82Vx-PhVtCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "ODTXlbKJVwEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "def main():\n",
        "    st.title(\"Aplikasi Peramalan Sederhana\")\n",
        "\n",
        "    # Tambahkan bagian untuk mengunggah file data jika diperlukan\n",
        "    uploaded_file = st.file_uploader(\"Unggah file CSV\", type=\"csv\")\n",
        "    if uploaded_file is not None:\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "        st.write(data)  # Menampilkan data yang diunggah\n",
        "\n"
      ],
      "metadata": {
        "id": "S1iXVRoht8-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "def main():\n",
        "    st.title(\"Aplikasi Streamlit Sederhana\")\n",
        "    st.write(\"Selamat datang di aplikasi Streamlit!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "1UBB42tsvEpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "LrITFH9fkB3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZJkNl7ttFEQ",
        "outputId": "35548a28-214a-4df5-c8ca-792df5f85cb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.0.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Port tempat aplikasi Streamlit berjalan (default: 8501)\n",
        "port = 80\n",
        "\n",
        "# Menjalankan ngrok dan mendapatkan URL publik\n",
        "public_url = ngrok.connect(port)\n",
        "print('Streamlit app is live at:', public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "6FGJN30z2GWQ",
        "outputId": "33dd541d-26f7-415a-fe8c-130feed44263"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2023-12-21T05:28:18+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=4a2771669103 err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2023-12-21T05:28:18+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2023-12-21T05:28:18+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2023-12-21T05:28:18+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c2b2e30820c1>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Menjalankan ngrok dan mendapatkan URL publik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Streamlit app is live at:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating tunnel with options: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             raise PyngrokNgrokError(\"The ngrok process errored on start: {}.\".format(ngrok_process.startup_error),\n\u001b[0m\u001b[1;32m    396\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    }
  ]
}